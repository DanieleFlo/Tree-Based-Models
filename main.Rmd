---
title: "Untitled"
output: pdf_document
date: "2025-04-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduzione



### Librerie
```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(scales)
library(randomForest) 
library(caret)
library(dplyr)
library(gbm)
```


# Import del dataset e analisi preliminare

```{r}
ds <- read.csv("StudentPerformanceFactors.csv")
ds = data.frame(ds)

# Lista di variabili categoriali
categorical_vars <- c(
  "Parental_Involvement", "Access_to_Resources", "Extracurricular_Activities",
  "Motivation_Level", "Internet_Access", "Family_Income", "Teacher_Quality",
  "School_Type", "Peer_Influence", "Learning_Disabilities",
  "Parental_Education_Level", "Distance_from_Home", "Gender"
)

ds[categorical_vars] <- lapply(ds[categorical_vars], factor)

head(ds)
```

#### Descrizione delle variabili
```{r}
colnames(ds)
```

*   **Hours_Studied**	Numero di ore spese studiando a settimana.
*   **Attendance** Percentuale di lezioni frequentate.
*   **Parental_Involvement** Livello di coinvolgimento genitoriale nella formazione dello studente (Low, Medium, High).
*   **Access_to_Resources**	Disponibilità di risorse educative(Low, Medium, High).
*   **Extracurricular_Activities** Partecipazione ad attività extracurriculari (Yes, No).
*   **Sleep_Hours**	Numero medio di ore di sonno a notte.
*   **Previous_Scores**	Punteggio degli esami precedenti.
*   **Motivation_Level** Livello di motivazione dello studente (Low, Medium, High).
*   **Internet_Access**	Disponibilità di accesso ad Internet (Yes, No).
*   **Tutoring_Sessions**	Numero di sessioni di tutoraggio frequentata al mese.
*   **Family_Income**	Livello di reddito familiare (Low, Medium, High).
*   **Teacher_Quality**	Qualità dell'insegnamento (Low, Medium, High).
*   **School_Type**	Tipo di scuola frequentata (Public, Private).
*   **Peer_Influence**	Influenza dei pari sulla performance accademica (Positive, Neutral, Negative).
*   **Physical_Activity**	Numero medio di ore di attività fisica a settimana.
*   **Learning_Disabilities**	Presenza di difficoltà di apprendimento (Yes, No).
*   **Parental_Education_Level**	Livello più alto di educazione dei genitori (High School, College, Postgraduate).
*   **Distance_from_Home**	Distanza da casa a scuola (Near, Moderate, Far).
*  **Gender**	Genere dello studente (Male, Female).
*   **Exam_Score**	Punteggio dell' esame finale.



```{r}
summary(ds)
```


```{r}
str(ds)
```


```{r}
ggplot(ds, aes(x = Exam_Score)) +
  geom_histogram(
    binwidth = 3,               # larghezza del bin; modificala a seconda della granularità desiderata
    fill     = "skyblue",       # colore interno delle barre
    color    = "white"          # colore del bordo delle barre
  ) +
  labs(
    x     = "Exam Score",
    y     = "Frequenza",
    title = "Istogramma di Exam_score"
  ) +
  theme_minimal(base_size = 14)
```



Trasformiamo la variabile Exam_Score in un variabile categorica.
```{r}

ds_2 = ds

ds_2$Categorical_Exam_Score <- cut(
  ds$Exam_Score,
  breaks = c(54, 64, 67, 70, 102),
  labels = c("Sufficiente", "Basso", "Medio", "Alto"),
  include.lowest = FALSE,
  right = TRUE
)


ds$Categorical_Exam_Score <- cut(
  ds$Exam_Score,
  breaks = c(54, 61, 64, 67, 70, 73, 102),
  labels = c("Quasi-Sufficiente", "Basso", "Medio-Basso", "Medio", "Medio-Alto", "Alto"),
  include.lowest = FALSE,
  right = TRUE
)

ggplot(ds, aes(x = Categorical_Exam_Score, 
               fill = Categorical_Exam_Score)) +
  # barre con proporzione
  geom_bar(
    aes(y = after_stat(count) / sum(after_stat(count))),
    stat = "count",
    width = 0.7,
    show.legend = FALSE
  ) +
  # percentuali sopra le barre
  geom_text(
    aes(
      label = percent(after_stat(count) / sum(after_stat(count)), accuracy = 1),
      y     = after_stat(count) / sum(after_stat(count))
    ),
    stat = "count",
    vjust = -0.5
  ) +
  # scala y in percentuale e un po’ di spazio in alto
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    expand = expansion(mult = c(0, 0.05))
  ) +
  labs(
    x     = "Categoria punteggio d'esame",
    y     = "Percentuale di osservazioni",
    title = "Distribuzione normalizzata di Categorical Exam Score"
  ) +
  theme_minimal(base_size = 14)

ggplot(ds_2, aes(x = Categorical_Exam_Score, 
               fill = Categorical_Exam_Score)) +
  # barre con proporzione
  geom_bar(
    aes(y = after_stat(count) / sum(after_stat(count))),
    stat = "count",
    width = 0.7,
    show.legend = FALSE
  ) +
  # percentuali sopra le barre
  geom_text(
    aes(
      label = percent(after_stat(count) / sum(after_stat(count)), accuracy = 1),
      y     = after_stat(count) / sum(after_stat(count))
    ),
    stat = "count",
    vjust = -0.5
  ) +
  # scala y in percentuale e un po’ di spazio in alto
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    expand = expansion(mult = c(0, 0.05))
  ) +
  labs(
    x     = "Categoria punteggio d'esame",
    y     = "Percentuale di osservazioni",
    title = "Distribuzione normalizzata di Categorical Exam Score"
  ) +
  theme_minimal(base_size = 14)
```


## Divisione del dataset in train e test

```{r}
set.seed(123)
ds$Exam_Score= NULL

trainIndex <- createDataPartition(ds$Categorical_Exam_Score, 
                                  p    = 0.5,
                                  list = FALSE)

train <- ds[ trainIndex, ]
test  <- ds[-trainIndex, ]
```

```{r}
set.seed(123)
ds_2$Exam_Score= NULL

trainIndex_2 <- createDataPartition(ds_2$Categorical_Exam_Score, 
                                  p    = 0.5,
                                  list = FALSE)

train_2 <- ds_2[ trainIndex, ]
test_2  <- ds_2[-trainIndex, ]
```


# Analisi

## Data aug

ROSE (Random Over‑Sampling Examples) è una tecnica di resampling per riequilibrare dataset sbilanciati, particolarmente utile quando la classe minoritaria ha pochissimi esempi rispetto alla classe maggioritaria. A differenza del semplice oversampling (che duplica casi esistenti) o del SMOTE (che interpola tra i vicini), ROSE sintetizza nuovi esempi attorno alle osservazioni reali utilizzando una densità di kernel multivariata, preservando la struttura locale dei dati.
```{r}
library(smotefamily)

table(train$Categorical_Exam_Score)

# Supponiamo: "Quasi-Sufficiente" e "Alto" sono le classi con pochi esempi
rare_classes <- c("Quasi-Sufficiente", "Alto")
# funzione di oversampling per una singola classe rara

apply_smote_to_class <- function(data, class_target, rate = 2) {
  # Crea etichetta binaria
  data$binary_target <- ifelse(data$Categorical_Exam_Score == class_target, 1, 0)
  
  # SMOTE lavora solo su variabili numeriche → isoliamo features numeriche
  x_vars <- data %>% select(where(is.numeric))
  y_bin  <- data$binary_target
  
  # Applica SMOTE (genererà nuovi esempi della classe 1)
  smote_out <- SMOTE(x_vars, y_bin, K = 12, dup_size = rate)
  
  # Recupera solo i sintetici generati (classe = 1)
  synthetic <- smote_out$syn_data %>% 
    mutate(Categorical_Exam_Score = class_target)
  
  # Rimuove colonna target binaria
  synthetic <- synthetic %>% select(-class)
  
  return(synthetic)
}

# Applichiamo SMOTE a ciascuna classe rara
synthetics <- lapply(rare_classes, function(cl) {
  apply_smote_to_class(train, class_target = cl, rate = 2)  # dup_size controlla quanto ne vuoi
})

# Combiniamo i sintetici
synthetic_data <- bind_rows(synthetics)

# Unisci al train originale
train_augmented <- bind_rows(train, synthetic_data)
train_augmented$binary_target = NULL


train_augmented$Categorical_Exam_Score = as.factor(train_augmented$Categorical_Exam_Score)

# Controlla la nuova distribuzione
table(train_augmented$Categorical_Exam_Score)


```
```{r}
impute_categorical_na_by_class_mode <- function(data, class_col, rare_classes) {
  # Identifica colonne categoriali (escluse quelle già numeriche o il target)
  categorical_cols <- data %>% select(where(~is.factor(.) || is.character(.))) %>% select(-all_of(class_col)) %>% colnames()
  
  for (cat_col in categorical_cols) {
    for (rare_class in rare_classes) {
      # Subset dei dati per la classe rara
      subset_class <- data %>% 
        filter(!!sym(class_col) == rare_class)
      
      # Calcola la moda ignorando gli NA
      mode_val <- subset_class %>%
        filter(!is.na(!!sym(cat_col))) %>%
        count(!!sym(cat_col), sort = TRUE) %>%
        slice(1) %>%
        pull(!!sym(cat_col))
      
      # Sostituisci NA con la moda solo per la classe rara corrente
      data <- data %>%
        mutate(!!sym(cat_col) := ifelse(
          is.na(!!sym(cat_col)) & (!!sym(class_col) == rare_class),
          mode_val,
          !!sym(cat_col)
        ))
    }
  }
  return(data)
}


train_augmented <- impute_categorical_na_by_class_mode(
  data = train_augmented,
  class_col = "Categorical_Exam_Score",
  rare_classes = rare_classes
)
```


```{r}
colSums(is.na(train_augmented))
```

```{r}
levels_list <- lapply(train[categorical_vars], function(x) {
  # se sono factor mantieni i livelli, altrimenti estrai i valori unici
  if (is.factor(x)) levels(x) else unique(as.character(x))
})
names(levels_list) <- categorical_vars

# 2) “Decodifica” in train_augmented gli indici numerici usando levels_list
for (var in categorical_vars) {
  # train_augmented[[var]] contiene un intero da 1 a length(levels_list[[var]])
  train_augmented[[var]] <- factor(
    train_augmented[[var]],
    levels = seq_along(levels_list[[var]]),
    labels = levels_list[[var]]
  )
}
```


```{r}
ggplot(train_augmented, aes(x = Categorical_Exam_Score, 
               fill = Categorical_Exam_Score)) +
  # barre con proporzione
  geom_bar(
    aes(y = after_stat(count) / sum(after_stat(count))),
    stat = "count",
    width = 0.7,
    show.legend = FALSE
  ) +
  # percentuali sopra le barre
  geom_text(
    aes(
      label = percent(after_stat(count) / sum(after_stat(count)), accuracy = 1),
      y     = after_stat(count) / sum(after_stat(count))
    ),
    stat = "count",
    vjust = -0.5
  ) +
  # scala y in percentuale e un po’ di spazio in alto
  scale_y_continuous(
    labels = percent_format(accuracy = 1),
    expand = expansion(mult = c(0, 0.05))
  ) +
  labs(
    x     = "Categoria punteggio d'esame",
    y     = "Percentuale di osservazioni",
    title = "Distribuzione normalizzata di Categorical Exam Score"
  ) +
  theme_minimal(base_size = 14)
```




## Random Forest

AUg

```{r}

p <- NCOL(train_augmented) - 1


set.seed(11)
 
rf_1_aug <- randomForest(Categorical_Exam_Score ~ . , data = train_augmented)
plot(rf_1_aug, col="#A20045", main="Random forest")
varImpPlot(rf_1_aug, main="Variable importance", pch = 19, color="#A20045")
```

```{r}
pred = predict(rf_1_aug)
confusionMatrix(pred, train_augmented$Categorical_Exam_Score)
```

```{r}
pred_test_aug = predict(rf_1_aug, newdata = test)
pred_test_aug <- factor(pred_test_aug, levels = levels(test$Categorical_Exam_Score))
confusionMatrix(pred_test_aug, test$Categorical_Exam_Score)
```

 NON aug

```{r}
set.seed(11)
rf_1 <- randomForest(Categorical_Exam_Score ~ . , data = train)
plot(rf_1, col="#A20045", main="Random forest")
varImpPlot(rf_1, main="Variable importance", pch = 19, color="#A20045")
```

```{r}
pred = predict(rf_1)
confusionMatrix(pred, train$Categorical_Exam_Score)
```

```{r}
pred_test = predict(rf_1, newdata = test)
confusionMatrix(pred_test, test$Categorical_Exam_Score)
```

```{r}
set.seed(11)
 
rf_2 <- randomForest(Categorical_Exam_Score ~ . , data = train_2)
plot(rf_2, col="#A20045", main="Random forest")
varImpPlot(rf_2, main="Variable importance", pch = 19, color="#A20045")
```

```{r}
set.seed(11)
pred_2 = predict(rf_2)
confusionMatrix(pred_2, train_2$Categorical_Exam_Score)
```

```{r}
set.seed(11)
pred_test_2 = predict(rf_2, newdata = test_2)
confusionMatrix(pred_test_2, test_2$Categorical_Exam_Score)
```

## Boosting

```{r, warning=FALSE}
set.seed(123)

boost.1 <- gbm(Categorical_Exam_Score ~ ., data = train, 
               distribution = "multinomial", n.trees = 2000, 
               interaction.depth = 4, shrinkage=0.1)
summary(boost.1)
```


```{r, warning=FALSE}

calculate_rowwise_error <- function(real_matrix, predicted_matrix, epsilon = 1e-15) {
  real_matrix <- as.matrix(real_matrix)
  predicted_matrix <- as.matrix(predicted_matrix)
  errors <- -rowSums(real_matrix * log(predicted_matrix))
  return(errors)
}

softmax <- function(logits) {
  stable_logits <- logits - max(logits) # Stabilità numerica
  exp_logits <- exp(stable_logits)
  return(exp_logits / sum(exp_logits))
}

n.trees.seq <- seq(from = 20, to = 2000, by = 30)

test_matrix_onehot <- model.matrix(~ test$Categorical_Exam_Score - 1)
train_matrix_onehot <- model.matrix(~ train$Categorical_Exam_Score - 1)

Yhat_logits_array_test <- predict(boost.1,
                             newdata = test,
                             n.trees = n.trees.seq 
                             )

Yhat_logits_array_train <- predict(boost.1, n.trees = n.trees.seq)



error_matrix_test <- matrix(NA, nrow = 1, ncol = length(n.trees.seq),
                       dimnames = list("MeanCrossEntropy",
                                       paste0("nTrees_", n.trees.seq)))
error_matrix_train <- matrix(NA, nrow = 1, ncol = length(n.trees.seq),
                       dimnames = list("MeanCrossEntropy",
                                       paste0("nTrees_", n.trees.seq)))

for (i in 1:length(n.trees.seq)) {
  current_n_trees <- n.trees.seq[i]
  
  current_logits_test <- Yhat_logits_array_test[,,i]
  current_probs_test <- t(apply(current_logits_test, 1, softmax))
  
  rowwise_errors_test <- calculate_rowwise_error(test_matrix_onehot, current_probs_test)
  mean_error_test <- mean(rowwise_errors_test)
  error_matrix_test[1, i] <- mean_error_test
  
  current_logits_train <- Yhat_logits_array_train[,,i]
  current_probs_train <- t(apply(current_logits_train, 1, softmax))
  
  rowwise_errors_train <- calculate_rowwise_error(train_matrix_onehot, current_probs_train)
  mean_error_train <- mean(rowwise_errors_train)
  error_matrix_train[1, i] <- mean_error_train

}


matplot(n.trees.seq, cbind(error_matrix_test[1, ], error_matrix_train[1, ]), pch=19 , col=c("#A20045","#00484D"),type="b",ylab="CrossEntropy",xlab="n. trees")
legend("topright",legend=c("Test Error","Train Error"),pch=19, col=c("#A20045","#00484D"))

```




```{r, warning=FALSE}
set.seed(123)
boost.2 <- gbm(Categorical_Exam_Score ~ ., data = train, distribution = "multinomial", n.trees = 1000, interaction.depth = 4, shrinkage=0.1, cv.folds = 5)

# Choose number of trees via xvalidation error
best.nt = gbm.perf(boost.2, method = "cv", plot.it = TRUE)
best.nt
```


```{r, warning=FALSE}
myd <- 1:7
test_matrix_onehot <- model.matrix(~ test$Categorical_Exam_Score - 1)
train_matrix_onehot <- model.matrix(~ train$Categorical_Exam_Score - 1)

myEval <- sapply(myd , function(x) {
  set.seed(123)
  boost.3 <- gbm(Categorical_Exam_Score ~ ., data = train, distribution = "multinomial", n.trees = 1000, interaction.depth = x, shrinkage=0.1, cv.folds = 5)
  
  ### best n. trees
  best.nt2 <- gbm.perf(boost.3, method = "cv", plot.it = FALSE) 
  
  
  Yhat_test <- predict(boost.3, newdata = test,n.trees = best.nt2)
  Yhat_test = t(apply(Yhat_test, 1, softmax))
  
  Yhat_train <- predict(boost.3, n.trees = best.nt2)
  Yhat_train = t(apply(Yhat_train, 1, softmax))
  
  ### MSE
  loss_train = mean(calculate_rowwise_error(train_matrix_onehot, Yhat_train))
  loss_test = mean(calculate_rowwise_error(test_matrix_onehot, Yhat_test))
  
  
  return(c(as.integer(best.nt2), round(loss_train,3), round(loss_test,3)))}
  )

myEval
```

```{r, warning=FALSE}
myEval_invertita <- t(myEval)

par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))

# Primo grafico: best.nt2
plot(1:7, myEval_invertita[,1], type = "b", pch = 19, col = "red",
     xlab = "depth", ylab = "best.nt2",
     main = "Andamento di best.nt2")

plot(1:7, myEval_invertita[,2], type = "b", pch = 19, col = "green", 
     xlab = "depth", ylab = "CrossEntropy", 
     main = "Andamento della CrossEntropy sul train e test", 
     ylim = range(c(myEval_invertita[,2], myEval_invertita[,3])))  # Impostare limiti y
lines(1:7, myEval_invertita[,3], type = "b", pch = 19, col = "blue", lwd = 2)

# Legenda
legend("topright",
       legend = c("Loss train", "Loss test"),
       col    = c("green", "blue"),
       pch    = 19)

indice_minimo <- which.min(myEval_invertita[,3])
best_nt= myEval_invertita[,1][indice_minimo]
best_depth =  indice_minimo
best_depth
best_nt
```
```{r, warning=FALSE}
set.seed(123)
boost.final <- gbm(Categorical_Exam_Score ~ ., data = train, distribution = "multinomial", n.trees = best_nt, interaction.depth = best_depth, shrinkage=0.1, cv.folds = 5)

# Effettua le previsioni sul set di training
predictions_train_probs <- predict(boost.final, newdata = train, n.trees = best_nt, type = "response")
predictions_train <- apply(predictions_train_probs, 1, which.max)
predicted_classes_train <- levels(train$Categorical_Exam_Score)[predictions_train]

# Calcola l'accuracy sul set di training
confusion_matrix_train <- confusionMatrix(factor(predicted_classes_train, levels = levels(train$Categorical_Exam_Score)), train$Categorical_Exam_Score)
accuracy_train <- confusion_matrix_train$overall["Accuracy"]
cat("Accuracy sul set di training:", accuracy_train, "\n")

# Effettua le previsioni sul set di test
predictions_test_probs <- predict(boost.final, newdata = test, n.trees = best_nt, type = "response")
predictions_test <- apply(predictions_test_probs, 1, which.max)
predicted_classes_test <- levels(test$Categorical_Exam_Score)[predictions_test]

# Calcola l'accuracy sul set di test
confusion_matrix_test <- confusionMatrix(factor(predicted_classes_test, levels = levels(test$Categorical_Exam_Score)), test$Categorical_Exam_Score)
accuracy_test <- confusion_matrix_test$overall["Accuracy"]
cat("Accuracy sul set di test:", accuracy_test, "\n")
```

```{r}
train_labels_factor <- factor(train$Categorical_Exam_Score, levels = levels(factor(predicted_classes_train)))
test_labels_factor <- factor(test$Categorical_Exam_Score, levels = levels(factor(predicted_classes_test)))

# Confusion matrix sul set di training
confusion_matrix_train <- confusionMatrix(factor(predicted_classes_train, levels = levels(train_labels_factor)), train_labels_factor)
print("Confusion Matrix - Training Set:")
print(confusion_matrix_train)

# Confusion matrix sul set di test
confusion_matrix_test <- confusionMatrix(factor(predicted_classes_test, levels = levels(test_labels_factor)), test_labels_factor)
print("Confusion Matrix - Test Set:")
print(confusion_matrix_test)
```

```{r}
summary(boost.final)
```

```{r}
plot(boost.final$train.error, type = "l", xlab = "Numero di alberi", ylab = "Errore di training")
lines(boost.final$cv.error, col = "red")
legend("topright", legend = c("Training Error", "Cross-Validation Error"), col = c("black", "red"), lty = 1)
```








## CART

```{r}
require(rpart)
library(rpart.plot)

mod0<- rpart::rpart(Categorical_Exam_Score~., data= train, method="class")
mod0
```

```{r}
rpart.plot::rpart.plot(mod0)
```


```{r}
caret::confusionMatrix(table(predicted = predict(mod0, type = "class"), actual = train$Categorical_Exam_Score), positive = "1")
```

```{r}
mytab <- table(predicted = predict(mod0, type = "class"), actual = train$Categorical_Exam_Score)
mctest = (abs(mytab[2,1] - mytab[1,2]) -1)^2/(mytab[2,1] + mytab[1,2])
mctest
pchisq(mctest,1, lower.tail = FALSE)
mcnemar.test(table(predicted = predict(mod0, type = "class"), actual = train$Categorical_Exam_Score), correct = TRUE)
```

Use of the loss matrix

```{r}
m = matrix(c(0,0,0,0,0,1.0,
             0,0,0,0,0.2,0, 
             0,0,0,0.1,0,0,
             0,0,0.1,0,0,0, 
             0,0.2,0,0,0,0, 
             1.0,0,0,0,0,0),
           byrow=TRUE, nrow=6)
m
mod0_loss<- rpart::rpart(Categorical_Exam_Score~., data= train, method="class", parms=list(loss=m))
mod0_loss
```
```{r}
rpart.plot::rpart.plot(mod0_loss)
```
```{r}
caret::confusionMatrix(table(predicted = predict(mod0_loss, type = "class"), actual =  train$Categorical_Exam_Score), positive="1")
```







# Conclusioni